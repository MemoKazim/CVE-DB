from bs4 import BeautifulSoup as soup
from lib.colors import bcolors
import threading
import requests
import time
import re


# SOURCES = {
#     1: "https://nvd.nist.gov/vuln/detail/CVE-XXXX-XXXX",
#     2: "https://www.exploit-db.com/search?cve=XXXX-XXXX",
#     3: "https://www.cve.org/CVERecord?id=CVE-XXXX-XXXX",
#     4: "https://vulners.com/cve/CVE-XXXX-XXXX",
#     5: "https://vulmon.com/vulnerabilitydetails?qid=CVE-XXXX-XXXX",
# }

is_active = True

def wheel_scrap(source):
    print(f"{bcolors.FG_YELLOW}[│] Scrapping from {source}...", end="")
    while is_active:
        for _ in ["/", "─", "\\", "│"]:
            print(f"\r[{_}] Scrapping from {source}...", end="")
            time.sleep(0.1)
    print(f"\r{bcolors.FG_GREEN}[+] Scrapping from {source} FINISHED!{bcolors.END}")

def mitre(cve):
    url = f"https://cve.mitre.org/cgi-bin/cvename.cgi?name={cve}"
    req = requests.get(url)

    if req.status_code != 200:
        print("Can't get response, GO check your internet connection")
        return

    html = soup(req.text, "html.parser")
    table = html.find("div",{"id":"GeneratedTable"})
    if table == None:
        print("CVE NOT FOUND")
        return
    trs = table.find_all("tr")

    description = trs[3].find("td").get_text()
    assigning_cna = trs[8].find("td").get_text()
    date_created = trs[10].find("td").get_text()
    references = trs[6].find("td").get_text()
    return {
        "description":description,
        "assigning_cna":assigning_cna,
        "date_created": date_created,
        "references": references,
    }

def nist(cve):
    url = f"https://nvd.nist.gov/vuln/detail/{cve}"
    req = requests.get(url)

    if not req.status_code:
        print("Can't get response, GO check your internet connection")
        return

    html = soup(req.text, "html.parser")
    # print(html)
    try:
        description = html.find("h3",{"id":"vulnDescriptionTitle"}).find_next_sibling().get_text()
    except:
        description = None

    try:
        base_score = html.find("span",{"class":"severityDetail"}).get_text()
    except:
        base_score = None

    try:
        vector = html.find("span",{"data-testid":"vuln-cvss3-nist-vector"}).get_text()
    except:
        vector = None

    try:
        knvt = html.find("div", {"id": "vulnCisaExploit"}).find_all("td")
    except:
        knvt = None

    # print(knvt)
    if knvt: 
        known_vul = {
            "name": knvt[0].get_text(),
            "data_add": knvt[1].get_text(),
            "due_date": knvt[2].get_text(),
            "action": knvt[3].get_text(),
        }
    else:
        known_vul = None

    #print weakness Enumeration
    weakenum = html.find("table", {"data-testid": "vuln-CWEs-table"}).find_all("tr")
    weak_enum_list = []
    for weak in weakenum[1:]:
        tds = weak.find_all("td")
        weak_enum_list.append({
            "cwe_id": tds[0].get_text().strip(),
            "cwe_name": tds[1].get_text().strip(),
            "cwe_source": tds[2].get_text().strip(),
        })
    
    references = html.find("table",{"class":"table table-striped table-condensed table-bordered detail-table"}).find("tbody").find_all("tr")
    ref_list = []

    for ref in references:
        # print("ref is ",ref)
        ref_tds = ref.find_all("td")
        resources = ref_tds[1].find_all("span",{"class":"badge"})
        resources = [res.get_text().strip() for res in resources]
        if 'Vendor Advisory' in resources:
            ref_list.insert(0,{
                "hyperlink":ref_tds[0].get_text(),
                "resource":resources,
            })
            continue
        # print(resources)
        ref_list.append({
        "hyperlink":ref_tds[0].get_text(),
        "resource":resources,
    }) 
    # print(json.dumps(ref_list, indent=2))
    return {
        "ref_list":ref_list,
        "weak_enum_list":weak_enum_list,
        "known_vul":known_vul,
        "vector": vector,
        "description":description,
        "base_score":base_score,
    }

def vulmon(cve):
    cve = cve.upper()

    res = requests.get(f"https://vulmon.com/vulnerabilitydetails?qid={cve}")
    html = soup(res.text, "html.parser")
    segments = html.find_all("div", {"class":"ui segment"})
    if len(segments) == 1:
        return
    degree = segments[0].find("div", {"class":"ui large statistics"}).find("div", {"class":"value"}).get_text()
    overview = re.sub(r'(\n ?)+',r'\n',segments[0].find("div", {"class":"content"}).get_text().strip())
    overview = re.sub(r'(\t)+',r'\t',overview)
    overview = re.sub(r'( ){2,}',r'',overview)
    overview = re.sub(r'Subscribe.*',r'',overview)

    sections = {
        "degree": degree,
        "overview":overview,
        "description": None,
        "vulnerable_products":None,
        "vendor_advisories":None,
        "ics_advisories":None,
        "exploits": None,
        "mailing_list":None,
        # "metasploit_modules":None,
        "github_repos":None,
        "references":None
    }

    v_p = html.find("th",string="Vulnerable Product")
    if v_p!= None:
        v_p = v_p.find_parent().find_parent().find_parent().find("tbody")
        v_ps = v_p.find_all("tr")
        arr = []
        for v in v_ps:
            arr.append(v.find("p").get_text())
        sections["vulnerable_products"] = arr
    
    desc = html.find("h3", string="Vulnerability Summary")

    if desc != None:
        desc = desc.next_sibling.next_sibling.get_text()

        sections["description"] = desc

    advisory = html.find("h3", string="Vendor Advisories")  
    
    if advisory != None:
        advisory = advisory.next_sibling.next_sibling.find_all("div",{"class":"item"})
        arr = []
        for adv in advisory:
            a = adv.find("a")
            arr.append({"name":a.get_text(),"url":f"https://vulmon.com{a['href'].replace(' ','%20')}"})
        sections["vendor_advisories"] = arr

    advisory = html.find("h3", string="ICS Advisories")  
    
    if advisory != None:
        advisory = advisory.next_sibling.next_sibling.find_all("div",{"class":"item"})
        arr = []
        for adv in advisory:
            a = adv.find("a")
            arr.append({"name":a.get_text(),"url":f"https://vulmon.com{a['href'].replace(' ','%20')}"})

        sections["ics_advisories"] = arr

    advisory = html.find("h3", string="Exploits")  
    
    if advisory != None:
        advisory = advisory.next_sibling.next_sibling.find_all("div",{"class":"item"})
        arr = []
        for adv in advisory:
            a = adv.find("a")
            arr.append({"name":a.get_text(),"url":f"https://vulmon.com{a['href'].replace(' ','%20')}"})
        sections["exploits"] = arr

    advisory = html.find("h3", string="Mailing Lists")  
    
    if advisory != None:
        advisory = advisory.next_sibling.next_sibling.find_all("div",{"class":"item"})
        arr = []
        for adv in advisory:
            a = adv.find("a")
            arr.append({"name":a.get_text(),"url":f"https://vulmon.com{a['href'].replace(' ','%20')}"})
        sections["mailing_list"] = arr

    advisory = html.find("h3", string="Github Repositories")  
    
    if advisory != None:
        advisory = advisory.next_sibling.next_sibling.find_all("div",{"class":"item"})
        arr = []
        for adv in advisory:
            a = adv.find("a")
            arr.append(f"{a['href'][:-1]}")
        sections["github_repos"] = arr

    references = html.find("h3", string="References")  
    
    if references != None:
        references = references.next_sibling.next_sibling.find_all("a",{"class":"item"})
        arr = []
        for item in references:
            arr.append(item["href"])

        sections["references"] = arr
    

    return sections


def scrap(source, function):
    global is_active
    t1 = threading.Thread(target=function)
    t2 = threading.Thread(target=wheel_scrap,args=(source,))
    t1.start()
    t2.start()
    time.sleep(2)
    while t1.is_alive():
        pass
    is_active = False
    t2.join()
    is_active = True

def merge(arr):
    result = {}
    for source_result in arr:
        for k,v in source_result.items():
            result[k] = v
    return result